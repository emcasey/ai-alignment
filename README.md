# AI Alignment Sandbox

## Overview
This repository is home to the AI Alignment Sandbox, a suite of interactive scenarios designed to educate users about reinforcement learning and the complexities of AI alignment. It's inspired by the work of AI researchers like Paul Christiano, founder of alignment.org. 

## Scenarios
- The Chemist: Safe Exploration
- The Urban Planner: Scalable Oversight
- Greenthumb: Avoiding Negative Side Effects

## Acknowledgments
Credits to Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man√©, whose paper on Contrete problems in AI safety as inspired these scenarios. 
