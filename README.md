# AI Alignment Sandbox

## Overview
This repository is home to the AI Alignment Sandbox, a suite of interactive scenarios designed to educate users about the complexities of AI alignment. It's inspired by the work of AI researchers and provides a hands-on approach to learning about AI safety.

## Scenarios
- The Chemist: Safe Exploration
- The Doctor: Robustness to Distributional Shift
- The Urban Planner: Scalable Oversight
- The Art Critic: Avoiding Reward Hacking
- Greenthumb: Avoiding Negative Side Effects

## Educational Value
Interactive elements and detailed scenario setups provide users with a practical understanding of AI alignment challenges.

## Contributions
Guidelines for contributing to this project.

## Acknowledgments
Credits to Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man√©, whose paper on Contrete problems in AI safety as inspired these scenarios. 

## Getting Started
Clone this repo and navigate to each scenario's directory: